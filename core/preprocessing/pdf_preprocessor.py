#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pdf_preprocessor.py
PDFé¢„å¤„ç†ç³»ç»Ÿ - æ‰¹é‡è§£æPDFæ–‡ä»¶ï¼Œæå–åœ°å€å’Œåæ ‡
å»ºç«‹ç´¢å¼•æ•°æ®åº“ä¾›å®æ—¶æŸ¥è¯¢ä½¿ç”¨
"""

import os
import sys
import json
import sqlite3
import pickle
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from dataclasses import dataclass, asdict
import hashlib

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "app" / "processors"))

import numpy as np
import cv2
from PIL import Image
import fitz  # PyMuPDF
import io

# OCRå¼•æ“å¯¼å…¥
try:
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False

from rapidfuzz import fuzz, process
import jieba

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ========================= æ•°æ®ç»“æ„ =========================

@dataclass
class AddressEntry:
    """åœ°å€æ¡ç›®"""
    id: str                          # å”¯ä¸€ID
    text: str                        # åœ°å€æ–‡æœ¬
    normalized_text: str             # æ ‡å‡†åŒ–åœ°å€æ–‡æœ¬
    bbox: Tuple[int, int, int, int]  # è¾¹ç•Œæ¡† (x1, y1, x2, y2)
    center_point: Tuple[int, int]    # ä¸­å¿ƒç‚¹åæ ‡
    confidence: float                # ç½®ä¿¡åº¦
    pdf_path: str                    # PDFæ–‡ä»¶è·¯å¾„
    page_num: int                    # é¡µç 
    prefecture: str                  # éƒ½é“åºœå¿
    city: str                        # å¸‚åŒºç”ºæ‘  
    district: str                    # ç”ºä¸ç›®
    sub_district: str                # å…·ä½“å°åŒºåŸŸ
    ocr_method: str                  # OCRæ–¹æ³•
    created_at: str                  # åˆ›å»ºæ—¶é—´

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    pdf_path: str
    total_addresses: int
    processing_time: float
    success: bool
    error_message: str = ""

# ========================= åœ°å€æ ‡å‡†åŒ–å™¨ =========================

class JapaneseAddressNormalizer:
    """æ—¥æ–‡åœ°å€æ ‡å‡†åŒ–å™¨"""
    
    def __init__(self):
        # æ•°å­—æ˜ å°„
        self.number_map = {
            'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
            'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
            'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
            'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10',
            'ï¼': '-', 'âˆ’': '-', 'â€': '-', 'â€‘': '-', 'â€’': '-'
        }
        
        # åœ°å€å…³é”®è¯
        self.address_keywords = [
            'ä¸ç›®', 'ç•ªåœ°', 'ç•ª', 'å·', 'ç”º', 'åŒº', 'å¸‚', 'åºœ', 'çœŒ',
            'ä¸', 'åœ°', 'ï¼‘', 'ï¼’', 'ï¼“', 'ï¼”', 'ï¼•', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼™', 'ï¼'
        ]
        
        # åˆå§‹åŒ–jieba
        jieba.initialize()
    
    def normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        if not text:
            return ""
        
        # Unicodeæ ‡å‡†åŒ–
        import unicodedata
        text = unicodedata.normalize('NFKC', text)
        
        # è½¬æ¢æ•°å­—
        for full, half in self.number_map.items():
            text = text.replace(full, half)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text.strip()
    
    def extract_address_components(self, text: str) -> Dict[str, str]:
        """æå–åœ°å€ç»„ä»¶"""
        normalized = self.normalize_text(text)
        components = {}
        
        # æå–æ•°å­—æ¨¡å¼
        import re
        
        # ä¸ç›®
        chome_match = re.search(r'(\d+)ä¸ç›®', normalized)
        if chome_match:
            components['chome'] = chome_match.group(1)
        
        # ç•ªåœ°
        banchi_match = re.search(r'(\d+)ç•ªåœ°?', normalized)
        if banchi_match:
            components['banchi'] = banchi_match.group(1)
        
        # å·
        go_match = re.search(r'(\d+)å·', normalized)
        if go_match:
            components['go'] = go_match.group(1)
        
        # è¿ç»­æ•°å­—æ¨¡å¼ (1-2-3)
        number_pattern = re.search(r'(\d+[-â€]\d+[-â€]\d+)', normalized)
        if number_pattern:
            components['full_number'] = number_pattern.group(1)
        
        return components
    
    def is_valid_address(self, text: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆåœ°å€"""
        if not text or len(text.strip()) < 2:
            return False
        
        normalized = self.normalize_text(text)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åœ°å€å…³é”®è¯
        for keyword in self.address_keywords:
            if keyword in normalized:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—
        if any(char.isdigit() for char in normalized):
            return True
        
        return False

# ========================= OCRå¤„ç†å™¨ =========================

class BatchOCRProcessor:
    """æ‰¹é‡OCRå¤„ç†å™¨"""
    
    def __init__(self, use_gpu: bool = True):
        self.use_gpu = use_gpu
        self.normalizer = JapaneseAddressNormalizer()
        self._init_ocr_engines()
    
    def _init_ocr_engines(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        self.ocr_engines = {}
        
        # PaddleOCR
        if PADDLEOCR_AVAILABLE:
            try:
                self.ocr_engines['paddleocr'] = PaddleOCR(
                    use_angle_cls=True,
                    lang='japan',
                    use_gpu=self.use_gpu,
                    show_log=False
                )
                logger.info("âœ… PaddleOCR åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"PaddleOCR åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # EasyOCR
        if EASYOCR_AVAILABLE:
            try:
                self.ocr_engines['easyocr'] = easyocr.Reader(['ja', 'en'], gpu=self.use_gpu, verbose=False)
                logger.info("âœ… EasyOCR åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"EasyOCR åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # Tesseract
        if TESSERACT_AVAILABLE:
            try:
                pytesseract.get_tesseract_version()
                self.ocr_engines['tesseract'] = 'available'
                logger.info("âœ… Tesseract å¯ç”¨")
            except Exception as e:
                logger.error(f"Tesseract ä¸å¯ç”¨: {e}")
    
    def process_pdf(self, pdf_path: str, prefecture: str, city: str, district: str) -> List[AddressEntry]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        addresses = []
        
        try:
            # æ‰“å¼€PDF
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # è½¬æ¢ä¸ºå›¾åƒ
                zoom = 2.0  # æé«˜åˆ†è¾¨ç‡
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat, alpha=False)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                img_data = pix.tobytes("png")
                img = Image.open(io.BytesIO(img_data)).convert('RGB')
                image = np.array(img)
                
                # OCRè¯†åˆ«
                page_addresses = self._extract_addresses_from_image(
                    image, pdf_path, page_num, prefecture, city, district
                )
                addresses.extend(page_addresses)
            
            doc.close()
            logger.info(f"å¤„ç†å®Œæˆ {pdf_path}: {len(addresses)} ä¸ªåœ°å€")
            
        except Exception as e:
            logger.error(f"å¤„ç†PDFå¤±è´¥ {pdf_path}: {e}")
        
        return addresses
    
    def _extract_addresses_from_image(self, image: np.ndarray, pdf_path: str, page_num: int,
                                    prefecture: str, city: str, district: str) -> List[AddressEntry]:
        """ä»å›¾åƒä¸­æå–åœ°å€"""
        addresses = []
        
        # ä½¿ç”¨æœ€ä½³å¯ç”¨çš„OCRå¼•æ“
        if 'paddleocr' in self.ocr_engines:
            addresses.extend(self._extract_with_paddleocr(image, pdf_path, page_num, prefecture, city, district))
        elif 'easyocr' in self.ocr_engines:
            addresses.extend(self._extract_with_easyocr(image, pdf_path, page_num, prefecture, city, district))
        elif 'tesseract' in self.ocr_engines:
            addresses.extend(self._extract_with_tesseract(image, pdf_path, page_num, prefecture, city, district))
        
        return addresses
    
    def _extract_with_paddleocr(self, image: np.ndarray, pdf_path: str, page_num: int,
                               prefecture: str, city: str, district: str) -> List[AddressEntry]:
        """ä½¿ç”¨PaddleOCRæå–åœ°å€"""
        addresses = []
        
        try:
            results = self.ocr_engines['paddleocr'].ocr(image, cls=True)
            
            if results and results[0]:
                for line in results[0]:
                    bbox_points = line[0]
                    text = line[1][0]
                    confidence = line[1][1]
                    
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆåœ°å€
                    if self.normalizer.is_valid_address(text):
                        # è®¡ç®—è¾¹ç•Œæ¡†
                        x_coords = [p[0] for p in bbox_points]
                        y_coords = [p[1] for p in bbox_points]
                        bbox = (int(min(x_coords)), int(min(y_coords)), 
                               int(max(x_coords)), int(max(y_coords)))
                        
                        # è®¡ç®—ä¸­å¿ƒç‚¹
                        center_x = (bbox[0] + bbox[2]) // 2
                        center_y = (bbox[1] + bbox[3]) // 2
                        
                        # åˆ›å»ºåœ°å€æ¡ç›®
                        address_entry = AddressEntry(
                            id=self._generate_id(pdf_path, page_num, text, bbox),
                            text=text,
                            normalized_text=self.normalizer.normalize_text(text),
                            bbox=bbox,
                            center_point=(center_x, center_y),
                            confidence=confidence,
                            pdf_path=pdf_path,
                            page_num=page_num,
                            prefecture=prefecture,
                            city=city,
                            district=district,
                            sub_district=self._extract_sub_district(text),
                            ocr_method='PaddleOCR',
                            created_at=datetime.now().isoformat()
                        )
                        addresses.append(address_entry)
        
        except Exception as e:
            logger.error(f"PaddleOCRå¤„ç†å¤±è´¥: {e}")
        
        return addresses
    
    def _extract_with_easyocr(self, image: np.ndarray, pdf_path: str, page_num: int,
                             prefecture: str, city: str, district: str) -> List[AddressEntry]:
        """ä½¿ç”¨EasyOCRæå–åœ°å€"""
        addresses = []
        
        try:
            results = self.ocr_engines['easyocr'].readtext(image)
            
            for bbox_points, text, confidence in results:
                if self.normalizer.is_valid_address(text):
                    # è®¡ç®—è¾¹ç•Œæ¡†
                    x_coords = [p[0] for p in bbox_points]
                    y_coords = [p[1] for p in bbox_points]
                    bbox = (int(min(x_coords)), int(min(y_coords)), 
                           int(max(x_coords)), int(max(y_coords)))
                    
                    # è®¡ç®—ä¸­å¿ƒç‚¹
                    center_x = (bbox[0] + bbox[2]) // 2
                    center_y = (bbox[1] + bbox[3]) // 2
                    
                    address_entry = AddressEntry(
                        id=self._generate_id(pdf_path, page_num, text, bbox),
                        text=text,
                        normalized_text=self.normalizer.normalize_text(text),
                        bbox=bbox,
                        center_point=(center_x, center_y),
                        confidence=confidence,
                        pdf_path=pdf_path,
                        page_num=page_num,
                        prefecture=prefecture,
                        city=city,
                        district=district,
                        sub_district=self._extract_sub_district(text),
                        ocr_method='EasyOCR',
                        created_at=datetime.now().isoformat()
                    )
                    addresses.append(address_entry)
        
        except Exception as e:
            logger.error(f"EasyOCRå¤„ç†å¤±è´¥: {e}")
        
        return addresses
    
    def _extract_with_tesseract(self, image: np.ndarray, pdf_path: str, page_num: int,
                               prefecture: str, city: str, district: str) -> List[AddressEntry]:
        """ä½¿ç”¨Tesseractæå–åœ°å€"""
        addresses = []
        
        try:
            config = r'--oem 3 --psm 6 -l jpn+eng'
            data = pytesseract.image_to_data(image, config=config, output_type=pytesseract.Output.DICT)
            
            for i in range(len(data['text'])):
                text = data['text'][i].strip()
                if text and data['conf'][i] > 30 and self.normalizer.is_valid_address(text):
                    bbox = (data['left'][i], data['top'][i],
                           data['left'][i] + data['width'][i],
                           data['top'][i] + data['height'][i])
                    
                    center_x = (bbox[0] + bbox[2]) // 2
                    center_y = (bbox[1] + bbox[3]) // 2
                    
                    address_entry = AddressEntry(
                        id=self._generate_id(pdf_path, page_num, text, bbox),
                        text=text,
                        normalized_text=self.normalizer.normalize_text(text),
                        bbox=bbox,
                        center_point=(center_x, center_y),
                        confidence=data['conf'][i] / 100.0,
                        pdf_path=pdf_path,
                        page_num=page_num,
                        prefecture=prefecture,
                        city=city,
                        district=district,
                        sub_district=self._extract_sub_district(text),
                        ocr_method='Tesseract',
                        created_at=datetime.now().isoformat()
                    )
                    addresses.append(address_entry)
        
        except Exception as e:
            logger.error(f"Tesseractå¤„ç†å¤±è´¥: {e}")
        
        return addresses
    
    def _generate_id(self, pdf_path: str, page_num: int, text: str, bbox: Tuple[int, int, int, int]) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        content = f"{pdf_path}_{page_num}_{text}_{bbox}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _extract_sub_district(self, text: str) -> str:
        """æå–å…·ä½“å°åŒºåŸŸä¿¡æ¯"""
        components = self.normalizer.extract_address_components(text)
        parts = []
        
        if 'chome' in components:
            parts.append(f"{components['chome']}ä¸ç›®")
        if 'banchi' in components:
            parts.append(f"{components['banchi']}ç•ª")
        if 'go' in components:
            parts.append(f"{components['go']}å·")
        if 'full_number' in components:
            parts.append(components['full_number'])
        
        return ''.join(parts) if parts else text

# ========================= æ•°æ®åº“ç®¡ç†å™¨ =========================

class AddressDatabase:
    """åœ°å€æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "address_index.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS addresses (
                    id TEXT PRIMARY KEY,
                    text TEXT NOT NULL,
                    normalized_text TEXT NOT NULL,
                    bbox_x1 INTEGER,
                    bbox_y1 INTEGER,
                    bbox_x2 INTEGER,
                    bbox_y2 INTEGER,
                    center_x INTEGER,
                    center_y INTEGER,
                    confidence REAL,
                    pdf_path TEXT NOT NULL,
                    page_num INTEGER,
                    prefecture TEXT,
                    city TEXT,
                    district TEXT,
                    sub_district TEXT,
                    ocr_method TEXT,
                    created_at TEXT
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_normalized_text ON addresses(normalized_text)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_prefecture_city_district ON addresses(prefecture, city, district)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pdf_path ON addresses(pdf_path)")
            
            conn.commit()
    
    def insert_addresses(self, addresses: List[AddressEntry]):
        """æ‰¹é‡æ’å…¥åœ°å€"""
        with sqlite3.connect(self.db_path) as conn:
            for addr in addresses:
                conn.execute("""
                    INSERT OR REPLACE INTO addresses VALUES (
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
                    )
                """, (
                    addr.id, addr.text, addr.normalized_text,
                    addr.bbox[0], addr.bbox[1], addr.bbox[2], addr.bbox[3],
                    addr.center_point[0], addr.center_point[1],
                    addr.confidence, addr.pdf_path, addr.page_num,
                    addr.prefecture, addr.city, addr.district, addr.sub_district,
                    addr.ocr_method, addr.created_at
                ))
            conn.commit()
    
    def search_addresses(self, query: str, prefecture: str = None, city: str = None, 
                        district: str = None, limit: int = 10) -> List[Dict]:
        """æœç´¢åœ°å€"""
        normalizer = JapaneseAddressNormalizer()
        normalized_query = normalizer.normalize_text(query)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = ["normalized_text LIKE ?"]
            params = [f"%{normalized_query}%"]
            
            if prefecture:
                conditions.append("prefecture = ?")
                params.append(prefecture)
            if city:
                conditions.append("city = ?")
                params.append(city)
            if district:
                conditions.append("district = ?")
                params.append(district)
            
            query_sql = f"""
                SELECT * FROM addresses 
                WHERE {' AND '.join(conditions)}
                ORDER BY confidence DESC
                LIMIT ?
            """
            params.append(limit)
            
            cursor = conn.execute(query_sql, params)
            results = []
            
            for row in cursor:
                result = {
                    'id': row['id'],
                    'text': row['text'],
                    'normalized_text': row['normalized_text'],
                    'bbox': (row['bbox_x1'], row['bbox_y1'], row['bbox_x2'], row['bbox_y2']),
                    'center_point': (row['center_x'], row['center_y']),
                    'confidence': row['confidence'],
                    'pdf_path': row['pdf_path'],
                    'page_num': row['page_num'],
                    'prefecture': row['prefecture'],
                    'city': row['city'],
                    'district': row['district'],
                    'sub_district': row['sub_district'],
                    'ocr_method': row['ocr_method'],
                    'created_at': row['created_at']
                }
                results.append(result)
            
            return results
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM addresses")
            total_addresses = cursor.fetchone()[0]
            
            cursor = conn.execute("SELECT COUNT(DISTINCT pdf_path) FROM addresses")
            total_pdfs = cursor.fetchone()[0]
            
            cursor = conn.execute("SELECT prefecture, city, district, COUNT(*) FROM addresses GROUP BY prefecture, city, district")
            area_stats = cursor.fetchall()
            
            return {
                'total_addresses': total_addresses,
                'total_pdfs': total_pdfs,
                'area_statistics': area_stats
            }

# ========================= ä¸»é¢„å¤„ç†å™¨ =========================

class PDFPreprocessor:
    """PDFé¢„å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self, data_dir: str = "rosenka_data", use_gpu: bool = True, max_workers: int = 4):
        self.data_dir = Path(data_dir)
        self.use_gpu = use_gpu
        self.max_workers = max_workers
        self.ocr_processor = BatchOCRProcessor(use_gpu=use_gpu)
        self.database = AddressDatabase()
        
    def process_directory(self, target_dir: str = None) -> Dict:
        """å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶"""
        if target_dir:
            process_path = self.data_dir / target_dir
        else:
            process_path = self.data_dir
        
        if not process_path.exists():
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {process_path}")
        
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = list(process_path.rglob("*.pdf"))
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        results = []
        total_addresses = 0
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for pdf_path in pdf_files:
                # è§£æè·¯å¾„è·å–åœ°ç†ä¿¡æ¯
                relative_path = pdf_path.relative_to(self.data_dir)
                path_parts = relative_path.parts
                
                if len(path_parts) >= 3:
                    prefecture = path_parts[0]
                    city = path_parts[1] 
                    district = path_parts[2] if len(path_parts) > 2 else ""
                    
                    future = executor.submit(
                        self._process_single_pdf, 
                        str(pdf_path), prefecture, city, district
                    )
                    futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result = future.result()
                    results.append(result)
                    if result.success:
                        total_addresses += result.total_addresses
                        logger.info(f"âœ… {result.pdf_path}: {result.total_addresses} ä¸ªåœ°å€")
                    else:
                        logger.error(f"âŒ {result.pdf_path}: {result.error_message}")
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥: {e}")
        
        processing_time = time.time() - start_time
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        report = {
            'total_pdfs': len(pdf_files),
            'successful_pdfs': sum(1 for r in results if r.success),
            'total_addresses': total_addresses,
            'processing_time': processing_time,
            'results': results,
            'database_stats': self.database.get_statistics()
        }
        
        logger.info(f"é¢„å¤„ç†å®Œæˆ: {report['successful_pdfs']}/{report['total_pdfs']} PDF, {total_addresses} ä¸ªåœ°å€, ç”¨æ—¶ {processing_time:.2f}ç§’")
        
        return report
    
    def _process_single_pdf(self, pdf_path: str, prefecture: str, city: str, district: str) -> ProcessingResult:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        start_time = time.time()
        
        try:
            addresses = self.ocr_processor.process_pdf(pdf_path, prefecture, city, district)
            
            if addresses:
                self.database.insert_addresses(addresses)
            
            processing_time = time.time() - start_time
            
            return ProcessingResult(
                pdf_path=pdf_path,
                total_addresses=len(addresses),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            return ProcessingResult(
                pdf_path=pdf_path,
                total_addresses=0,
                processing_time=processing_time,
                success=False,
                error_message=str(e)
            )
    
    def search(self, query: str, prefecture: str = None, city: str = None, 
               district: str = None, limit: int = 10) -> List[Dict]:
        """æœç´¢åœ°å€"""
        return self.database.search_addresses(query, prefecture, city, district, limit)

# ========================= å‘½ä»¤è¡Œæ¥å£ =========================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="PDFé¢„å¤„ç†ç³»ç»Ÿ")
    parser.add_argument("--data-dir", default="rosenka_data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--target-dir", help="ç›®æ ‡å­ç›®å½•")
    parser.add_argument("--gpu", action="store_true", help="ä½¿ç”¨GPUåŠ é€Ÿ")
    parser.add_argument("--workers", type=int, default=4, help="å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--search", help="æœç´¢åœ°å€")
    
    args = parser.parse_args()
    
    preprocessor = PDFPreprocessor(
        data_dir=args.data_dir,
        use_gpu=args.gpu,
        max_workers=args.workers
    )
    
    if args.search:
        # æœç´¢æ¨¡å¼
        results = preprocessor.search(args.search, limit=10)
        print(f"æœç´¢ç»“æœ ({len(results)}):")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['text']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            print(f"   ä½ç½®: {result['pdf_path']} ç¬¬{result['page_num']+1}é¡µ")
            print(f"   åæ ‡: {result['center_point']}")
            print()
    else:
        # é¢„å¤„ç†æ¨¡å¼
        print("ğŸš€ å¼€å§‹PDFé¢„å¤„ç†...")
        report = preprocessor.process_directory(args.target_dir)
        
        print("\nğŸ“Š å¤„ç†æŠ¥å‘Š:")
        print(f"æ€»PDFæ–‡ä»¶: {report['total_pdfs']}")
        print(f"æˆåŠŸå¤„ç†: {report['successful_pdfs']}")
        print(f"æå–åœ°å€: {report['total_addresses']}")
        print(f"å¤„ç†æ—¶é—´: {report['processing_time']:.2f}ç§’")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"preprocessing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

if __name__ == "__main__":
    main() 