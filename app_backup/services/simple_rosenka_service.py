#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
simple_rosenka_service.py
ç®€åŒ–ç‰ˆè·¯ç·šä¾¡å›³æŸ¥è¯¢APIæœåŠ¡
é¿å…å¤æ‚çš„ä¾èµ–é—®é¢˜ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½
"""

import os
import json
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Any
from pathlib import Path
import logging

# åŸºç¡€ä¾èµ–
import streamlit as st
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# æ•°æ®å¤„ç†
import numpy as np
import pandas as pd
from PIL import Image

# OCRå¼•æ“
try:
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False

import pytesseract
from rapidfuzz import fuzz

# PDFå¤„ç†
import fitz  # PyMuPDF

# å¯¼å…¥ç°æœ‰çš„å¤„ç†å™¨
from simple_processor import SimplePDFProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "rosenka_data"
CACHE_DIR = BASE_DIR / ".cache"

# åˆ›å»ºå¿…è¦ç›®å½•
CACHE_DIR.mkdir(exist_ok=True)

# ========================= æ•°æ®æ¨¡å‹ =========================

class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    prefecture: Optional[str] = Field(None, description="éƒ½é“åºœå¿")
    city: Optional[str] = Field(None, description="å¸‚åŒºç”ºæ‘")
    district: Optional[str] = Field(None, description="ç”ºä¸ç›®")
    similarity_threshold: float = Field(50, ge=0, le=100, description="ç›¸ä¼¼åº¦é˜ˆå€¼")
    max_results: int = Field(50, ge=1, le=200, description="æœ€å¤§ç»“æœæ•°")
    use_cache: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨ç¼“å­˜")

class SearchResponse(BaseModel):
    """æœç´¢å“åº”"""
    results: List[Dict[str, Any]]
    total_count: int
    search_time: float
    cache_used: bool
    timestamp: datetime

# ========================= ç®€åŒ–OCRå¼•æ“ =========================

class SimpleOCREngine:
    """ç®€åŒ–OCRå¼•æ“"""
    
    def __init__(self):
        self.engines = {}
        self._init_engines()
    
    def _init_engines(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        # PaddleOCR
        if PADDLEOCR_AVAILABLE:
            try:
                self.engines['paddleocr'] = PaddleOCR(
                    use_angle_cls=True,
                    lang='japan'
                )
                logger.info("PaddleOCR åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"PaddleOCR åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # Tesseract
        try:
            pytesseract.get_tesseract_version()
            self.engines['tesseract'] = 'available'
            logger.info("Tesseract å¯ç”¨")
        except Exception as e:
            logger.error(f"Tesseract ä¸å¯ç”¨: {e}")
    
    def extract_text(self, image: np.ndarray) -> List[tuple]:
        """æå–æ–‡æœ¬"""
        results = []
        
        # å°è¯•PaddleOCR
        if 'paddleocr' in self.engines:
            try:
                ocr_results = self.engines['paddleocr'].ocr(image, cls=True)
                if ocr_results and ocr_results[0]:
                    for line in ocr_results[0]:
                        bbox_points = line[0]
                        text = line[1][0]
                        confidence = line[1][1]
                        
                        # è½¬æ¢è¾¹ç•Œæ¡†æ ¼å¼
                        x_coords = [p[0] for p in bbox_points]
                        y_coords = [p[1] for p in bbox_points]
                        bbox = (int(min(x_coords)), int(min(y_coords)), 
                               int(max(x_coords)), int(max(y_coords)))
                        
                        results.append((text, bbox, confidence))
                return results
            except Exception as e:
                logger.error(f"PaddleOCRæå–å¤±è´¥: {e}")
        
        # å°è¯•Tesseract
        if 'tesseract' in self.engines:
            try:
                config = r'--oem 3 --psm 6 -l jpn'
                data = pytesseract.image_to_data(
                    image, config=config, output_type=pytesseract.Output.DICT
                )
                
                for i in range(len(data['text'])):
                    text = data['text'][i].strip()
                    if text:
                        bbox = (
                            data['left'][i],
                            data['top'][i],
                            data['left'][i] + data['width'][i],
                            data['top'][i] + data['height'][i]
                        )
                        confidence = data['conf'][i] / 100.0
                        results.append((text, bbox, confidence))
                return results
            except Exception as e:
                logger.error(f"Tesseractæå–å¤±è´¥: {e}")
        
        return results

# ========================= ç®€åŒ–æœç´¢æœåŠ¡ =========================

class SimpleSearchService:
    """ç®€åŒ–æœç´¢æœåŠ¡"""
    
    def __init__(self):
        self.ocr_engine = SimpleOCREngine()
        self.processor = SimplePDFProcessor(dpi=300)
        self.cache = {}
        
        # åˆå§‹åŒ–æ–‡ä»¶ç´¢å¼•
        self._init_file_index()
    
    def _init_file_index(self):
        """åˆå§‹åŒ–æ–‡ä»¶ç´¢å¼•"""
        logger.info("åˆå§‹åŒ–æ–‡ä»¶ç´¢å¼•...")
        
        if not DATA_DIR.exists():
            logger.warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
            return
        
        # æ‰«ææ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = list(DATA_DIR.glob("**/*.pdf"))
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        self.pdf_files = []
        for pdf_path in pdf_files:
            # è§£æè·¯å¾„ç»“æ„: rosenka_data/prefecture/city/district/file.pdf
            relative_path = pdf_path.relative_to(DATA_DIR)
            parts = relative_path.parts
            
            if len(parts) >= 4:  # è‡³å°‘æœ‰4çº§: éƒ½é“åºœçœŒ/å¸‚åŒºç”ºæ‘/ç”ºä¸ç›®/æ–‡ä»¶.pdf
                prefecture = parts[0]  # éƒ½é“åºœçœŒ
                city = parts[1]        # å¸‚åŒºç”ºæ‘
                district = parts[2]    # ç”ºä¸ç›®
                filename = parts[-1]   # PDFæ–‡ä»¶å
                
                self.pdf_files.append({
                    'path': str(pdf_path),
                    'prefecture': prefecture,
                    'city': city,
                    'district': district,
                    'filename': filename,
                    'full_address': f"{prefecture}{city}{district}",
                    'relative_path': str(relative_path)
                })
        
        logger.info(f"ç´¢å¼•äº† {len(self.pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    def search(self, query: str, **kwargs) -> List[Dict]:
        """æœç´¢åœ°å€ - é€å±‚ç²¾ç¡®åŒ¹é…"""
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{query}_{hashlib.md5(query.encode()).hexdigest()}"
        if kwargs.get('use_cache', True) and cache_key in self.cache:
            logger.info(f"ç¼“å­˜å‘½ä¸­: {query}")
            return self.cache[cache_key]
        
        results = []
        prefecture_filter = kwargs.get('prefecture')
        city_filter = kwargs.get('city')
        district_filter = kwargs.get('district')
        max_results = kwargs.get('max_results', 50)
        
        # è§£ææŸ¥è¯¢åœ°å€
        parsed_address = self._parse_address(query)
        logger.info(f"è§£æåœ°å€: {parsed_address}")
        
        # è¿‡æ»¤æ–‡ä»¶
        filtered_files = self.pdf_files
        
        if prefecture_filter:
            filtered_files = [f for f in filtered_files if f['prefecture'] == prefecture_filter]
        
        if city_filter:
            filtered_files = [f for f in filtered_files if f['city'] == city_filter]
        
        if district_filter:
            filtered_files = [f for f in filtered_files if f['district'] == district_filter]
        
        logger.info(f"æœç´¢æŸ¥è¯¢: {query}, è¿‡æ»¤åæ–‡ä»¶æ•°: {len(filtered_files)}")
        
        # é€å±‚ç²¾ç¡®åŒ¹é…
        for pdf_info in filtered_files:
            match_score = self._calculate_match_score(pdf_info, parsed_address)
            
            if match_score > 0:
                result = {
                    'pdf_path': pdf_info['path'],
                    'prefecture': pdf_info['prefecture'],
                    'city': pdf_info['city'],
                    'district': pdf_info['district'],
                    'filename': pdf_info['filename'],
                    'full_address': pdf_info['full_address'],
                    'similarity': match_score,
                    'method': 'hierarchical_match',
                    'match_type': 'hierarchical',
                    'relative_path': pdf_info['relative_path'],
                    'match_details': self._get_match_details(pdf_info, parsed_address)
                }
                results.append(result)
        
        # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        results = results[:max_results]
        
        # ç¼“å­˜ç»“æœ
        if kwargs.get('use_cache', True):
            self.cache[cache_key] = results
        
        search_time = time.time() - start_time
        
        return {
            'results': results,
            'total_count': len(results),
            'search_time': search_time,
            'cache_used': cache_key in self.cache,
            'timestamp': datetime.now()
        }
    
    def _parse_address(self, query: str) -> Dict[str, str]:
        """è§£æåœ°å€æŸ¥è¯¢ï¼Œæå–éƒ½é“åºœå¿ã€å¸‚åŒºç”ºæ‘ã€ç”ºä¸ç›®"""
        query = query.strip()
        parsed = {'prefecture': '', 'city': '', 'district': ''}
        
        # éƒ½é“åºœå¿å…³é”®è¯
        prefecture_keywords = [
            'åŒ—æµ·é“', 'é’æ£®çœŒ', 'å²©æ‰‹çœŒ', 'å®®åŸçœŒ', 'ç§‹ç”°çœŒ', 'å±±å½¢çœŒ', 'ç¦å³¶çœŒ',
            'èŒ¨åŸçœŒ', 'æ ƒæœ¨çœŒ', 'ç¾¤é¦¬çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ',
            'æ–°æ½ŸçœŒ', 'å¯Œå±±çœŒ', 'çŸ³å·çœŒ', 'ç¦äº•çœŒ', 'å±±æ¢¨çœŒ', 'é•·é‡çœŒ',
            'å²é˜œçœŒ', 'é™å²¡çœŒ', 'æ„›çŸ¥çœŒ', 'ä¸‰é‡çœŒ',
            'æ»‹è³€çœŒ', 'äº¬éƒ½åºœ', 'å¤§é˜ªåºœ', 'å…µåº«çœŒ', 'å¥ˆè‰¯çœŒ', 'å’Œæ­Œå±±çœŒ',
            'é³¥å–çœŒ', 'å³¶æ ¹çœŒ', 'å²¡å±±çœŒ', 'åºƒå³¶çœŒ', 'å±±å£çœŒ',
            'å¾³å³¶çœŒ', 'é¦™å·çœŒ', 'æ„›åª›çœŒ', 'é«˜çŸ¥çœŒ',
            'ç¦å²¡çœŒ', 'ä½è³€çœŒ', 'é•·å´çœŒ', 'ç†Šæœ¬çœŒ', 'å¤§åˆ†çœŒ', 'å®®å´çœŒ', 'é¹¿å…å³¶çœŒ', 'æ²–ç¸„çœŒ'
        ]
        
        # æŸ¥æ‰¾éƒ½é“åºœå¿
        for pref in prefecture_keywords:
            if pref in query:
                parsed['prefecture'] = pref
                break
        
        # æ”¹è¿›çš„å¸‚åŒºç”ºæ‘è¯†åˆ«
        # é¦–å…ˆå°è¯•ä»ç°æœ‰æ•°æ®ä¸­æŸ¥æ‰¾åŒ¹é…çš„å¸‚åŒºç”ºæ‘
        if parsed['prefecture']:
            available_cities = self._get_available_cities(parsed['prefecture'])
            
            # åœ¨æŸ¥è¯¢ä¸­æŸ¥æ‰¾åŒ¹é…çš„å¸‚åŒºç”ºæ‘
            for city in available_cities:
                if city in query:
                    parsed['city'] = city
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡å¼åŒ¹é…
        if not parsed['city']:
            city_patterns = ['å¸‚', 'åŒº', 'ç”º', 'æ‘']
            for pattern in city_patterns:
                # æŸ¥æ‰¾åŒ…å«è¯¥æ¨¡å¼çš„è¯
                words = query.split()
                for word in words:
                    if pattern in word and word != parsed['prefecture']:
                        parsed['city'] = word
                        break
                if parsed['city']:
                    break
        
        # æŸ¥æ‰¾ç”ºä¸ç›® (é€šå¸¸åŒ…å«æ•°å­—æˆ–ç‰¹å®šæ¨¡å¼)
        district_patterns = ['ä¸ç›®', 'ç•ªåœ°', 'æ¡', 'æ®µ']
        for pattern in district_patterns:
            if pattern in query:
                # æå–åŒ…å«è¯¥æ¨¡å¼çš„å®Œæ•´è¯
                words = query.split()
                for word in words:
                    if pattern in word:
                        parsed['district'] = word
                        break
                if parsed['district']:
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ç”ºä¸ç›®ï¼Œå°è¯•æå–å‰©ä½™éƒ¨åˆ†
        if not parsed['district']:
            remaining = query
            if parsed['prefecture']:
                remaining = remaining.replace(parsed['prefecture'], '').strip()
            if parsed['city']:
                remaining = remaining.replace(parsed['city'], '').strip()
            if remaining:
                parsed['district'] = remaining
        
        return parsed
    
    def _get_available_cities(self, prefecture: str) -> List[str]:
        """è·å–æŒ‡å®šéƒ½é“åºœå¿çš„å¯ç”¨å¸‚åŒºç”ºæ‘åˆ—è¡¨"""
        cities = set()
        for pdf_info in self.pdf_files:
            if pdf_info['prefecture'] == prefecture:
                cities.add(pdf_info['city'])
        return sorted(list(cities))
    
    def _calculate_match_score(self, pdf_info: Dict, parsed_address: Dict) -> int:
        """è®¡ç®—åŒ¹é…åˆ†æ•° (0-100)"""
        score = 0
        
        # éƒ½é“åºœå¿åŒ¹é… (æƒé‡40)
        if parsed_address['prefecture'] and pdf_info['prefecture'] == parsed_address['prefecture']:
            score += 40
        elif parsed_address['prefecture'] and parsed_address['prefecture'] in pdf_info['prefecture']:
            score += 30
        
        # å¸‚åŒºç”ºæ‘åŒ¹é… (æƒé‡35)
        if parsed_address['city'] and pdf_info['city'] == parsed_address['city']:
            score += 35
        elif parsed_address['city'] and parsed_address['city'] in pdf_info['city']:
            score += 25
        
        # ç”ºä¸ç›®åŒ¹é… (æƒé‡25)
        if parsed_address['district'] and pdf_info['district'] == parsed_address['district']:
            score += 25
        elif parsed_address['district'] and parsed_address['district'] in pdf_info['district']:
            score += 15
        
        # éƒ¨åˆ†åŒ¹é…å¥–åŠ±
        if parsed_address['prefecture'] and pdf_info['prefecture'].startswith(parsed_address['prefecture']):
            score += 5
        if parsed_address['city'] and pdf_info['city'].startswith(parsed_address['city']):
            score += 5
        if parsed_address['district'] and pdf_info['district'].startswith(parsed_address['district']):
            score += 5
        
        return score
    
    def _get_match_details(self, pdf_info: Dict, parsed_address: Dict) -> Dict:
        """è·å–åŒ¹é…è¯¦æƒ…"""
        details = {
            'prefecture_match': False,
            'city_match': False,
            'district_match': False,
            'exact_matches': 0,
            'partial_matches': 0
        }
        
        exact_matches = 0
        partial_matches = 0
        
        # æ£€æŸ¥éƒ½é“åºœå¿
        if parsed_address['prefecture']:
            if pdf_info['prefecture'] == parsed_address['prefecture']:
                details['prefecture_match'] = True
                exact_matches += 1
            elif parsed_address['prefecture'] in pdf_info['prefecture']:
                partial_matches += 1
        
        # æ£€æŸ¥å¸‚åŒºç”ºæ‘
        if parsed_address['city']:
            if pdf_info['city'] == parsed_address['city']:
                details['city_match'] = True
                exact_matches += 1
            elif parsed_address['city'] in pdf_info['city']:
                partial_matches += 1
        
        # æ£€æŸ¥ç”ºä¸ç›®
        if parsed_address['district']:
            if pdf_info['district'] == parsed_address['district']:
                details['district_match'] = True
                exact_matches += 1
            elif parsed_address['district'] in pdf_info['district']:
                partial_matches += 1
        
        details['exact_matches'] = exact_matches
        details['partial_matches'] = partial_matches
        
        return details
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_pdfs': len(self.pdf_files),
            'total_addresses': 0,  # ç®€åŒ–ç‰ˆæœ¬ä¸ç»Ÿè®¡
            'cache_hits': 0,
            'cache_misses': 0,
            'search_count': 0,
            'avg_response_time': 0,
            'last_updated': datetime.now()
        }
    
    def get_prefectures(self) -> List[str]:
        """è·å–éƒ½é“åºœå¿åˆ—è¡¨"""
        prefectures = set()
        for pdf_info in self.pdf_files:
            prefectures.add(pdf_info['prefecture'])
        return sorted(list(prefectures))
    
    def get_cities(self, prefecture: str) -> List[str]:
        """è·å–å¸‚åŒºç”ºæ‘åˆ—è¡¨"""
        cities = set()
        for pdf_info in self.pdf_files:
            if pdf_info['prefecture'] == prefecture:
                cities.add(pdf_info['city'])
        return sorted(list(cities))
    
    def get_districts(self, prefecture: str, city: str) -> List[str]:
        """è·å–ç”ºä¸ç›®åˆ—è¡¨"""
        districts = set()
        for pdf_info in self.pdf_files:
            if pdf_info['prefecture'] == prefecture and pdf_info['city'] == city:
                districts.add(pdf_info['district'])
        return sorted(list(districts))

# ========================= FastAPIåº”ç”¨ =========================

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="è·¯ç·šä¾¡å›³æŸ¥è¯¢API",
    description="ç®€åŒ–ç‰ˆé«˜æ€§èƒ½è·¯ç·šä¾¡å›³åœ°å€æŸ¥è¯¢æœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æœåŠ¡å®ä¾‹
search_service = None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global search_service
    search_service = SimpleSearchService()
    logger.info("è·¯ç·šä¾¡å›³æŸ¥è¯¢æœåŠ¡å¯åŠ¨å®Œæˆ")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­äº‹ä»¶"""
    logger.info("è·¯ç·šä¾¡å›³æŸ¥è¯¢æœåŠ¡å…³é—­å®Œæˆ")

def get_search_service() -> SimpleSearchService:
    """è·å–æœç´¢æœåŠ¡å®ä¾‹"""
    return search_service

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "è·¯ç·šä¾¡å›³æŸ¥è¯¢APIæœåŠ¡è¿è¡Œä¸­", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now()}

@app.post("/search", response_model=SearchResponse)
async def search_address(request: SearchRequest):
    """æœç´¢åœ°å€"""
    try:
        result = search_service.search(
            request.query,
            prefecture=request.prefecture,
            city=request.city,
            district=request.district,
            similarity_threshold=request.similarity_threshold,
            max_results=request.max_results,
            use_cache=request.use_cache
        )
        return SearchResponse(**result)
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return search_service.get_stats()

@app.get("/prefectures")
async def get_prefectures():
    """è·å–éƒ½é“åºœå¿åˆ—è¡¨"""
    return search_service.get_prefectures()

@app.get("/cities/{prefecture}")
async def get_cities(prefecture: str):
    """è·å–å¸‚åŒºç”ºæ‘åˆ—è¡¨"""
    return search_service.get_cities(prefecture)

@app.get("/districts/{prefecture}/{city}")
async def get_districts(prefecture: str, city: str):
    """è·å–ç”ºä¸ç›®åˆ—è¡¨"""
    return search_service.get_districts(prefecture, city)

@app.delete("/cache")
async def clear_cache():
    """æ¸…ç†ç¼“å­˜"""
    search_service.cache.clear()
    return {"message": "ç¼“å­˜æ¸…ç†å®Œæˆ"}

# ========================= ä¸»å‡½æ•° =========================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è·¯ç·šä¾¡å›³æŸ¥è¯¢æœåŠ¡")
    parser.add_argument("--host", default="127.0.0.1", help="ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç«¯å£å·")
    
    args = parser.parse_args()
    
    print("ğŸ—¾ è·¯ç·šä¾¡å›³æŸ¥è¯¢APIæœåŠ¡å¯åŠ¨ä¸­...")
    print(f"ğŸ“¡ æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    print(f"ğŸ“– APIæ–‡æ¡£: http://{args.host}:{args.port}/docs")
    print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    uvicorn.run(
        "simple_rosenka_service:app",
        host=args.host,
        port=args.port,
        reload=True
    )

if __name__ == "__main__":
    main() 